#!/usr/bin/env python

import logging
import simplejson as json
import signal

import click
from clickhouse_driver import Client
from raven import Client as RavenClient

from snuba import settings
from snuba.consumer import AbstractBatchWorker, BatchingKafkaConsumer
from snuba.writer import SnubaWriter, row_from_processed_event
from snuba import util


logger = logging.getLogger('snuba.writer')
sentry = RavenClient(dsn=settings.SENTRY_DSN)

DEFAULT_CLICKHOUSE_NODES = [
    'clickhouse-08b7387d',
    'clickhouse-a8ef8458',
    'clickhouse-649c2398',
    'clickhouse-f8e2348b'
]

# TODO: Move into click options
LOCAL_TABLE = 'sentry_local'
DIST_TABLE = 'sentry_dist'
CLUSTER = 'cluster1'
DATABASE = 'default'
LOCAL_TABLE_DEFINITION = util.get_table_definition(
    LOCAL_TABLE, util.get_replicated_engine(name=LOCAL_TABLE))
DIST_TABLE_DEFINITION = util.get_table_definition(DIST_TABLE, util.get_distributed_engine(
    cluster=CLUSTER,
    database=DATABASE,
    local_table=LOCAL_TABLE,
))


@click.command()
@click.option('--processed-events-topic', default='snuba',
              help='Topic to consume processed events from.')
@click.option('--consumer-group', default='snuba-writers',
              help='Consumer group used for consuming the processed events topic.')
@click.option('--bootstrap-server', default=settings.DEFAULT_BROKERS, multiple=True,
              help='Kafka bootstrap server to use (multiple allowed).')
@click.option('--clickhouse-server', default=DEFAULT_CLICKHOUSE_NODES, multiple=True,
              help='Clickhouse server to write to (multiple allowed).')
@click.option('--log-level', default='WARN', help='Logging level to use.')
@click.option('--dogstatsd-host', default='localhost', help='Host to send DogStatsD metrics to.')
@click.option('--dogstatsd-port', default=8125, type=int, help='Port to send DogStatsD metrics to.')
def run(processed_events_topic, consumer_group, bootstrap_server, clickhouse_server, log_level,
        dogstatsd_host, dogstatsd_port):
    logging.basicConfig(level=getattr(logging, log_level.upper()))

    metrics = util.create_metrics(dogstatsd_host, dogstatsd_port, 'snuba.writer')

    connections = [Client(node) for node in clickhouse_server]

    # ensure tables exist
    for conn in connections:
        conn.execute(LOCAL_TABLE_DEFINITION)
        conn.execute(DIST_TABLE_DEFINITION)

    writer = SnubaWriter(connections=connections, columns=settings.WRITER_COLUMNS, table=DIST_TABLE)

    class WriterWorker(AbstractBatchWorker):
        def process_message(self, message):
            return row_from_processed_event(json.loads(message.value))

        def flush_batch(self, batch):
            writer.write(batch)

        def shutdown(self):
            pass

    consumer = BatchingKafkaConsumer(
        processed_events_topic,
        worker=WriterWorker(),
        max_batch_size=100000,
        max_batch_time=30 * 1000,
        metrics=metrics,
        bootstrap_servers=bootstrap_server,
        group_id=consumer_group,
        auto_offset_reset='earliest',
        consumer_timeout_ms=1000,
    )

    def handler(signum, frame):
        consumer.signal_shutdown()

    signal.signal(signal.SIGINT, handler)

    consumer.run()


if __name__ == '__main__':
    run()
